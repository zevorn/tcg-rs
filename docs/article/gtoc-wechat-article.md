# 一个人、两个 Agent、五天时间：用 AI 重写 QEMU 二进制翻译引擎的幕后故事

> GTOC 闭门讨论会实录 | 泽文 × 社区的朋友们

---

春节假期的最后一天晚上，一场线上闭门讨论会悄然开始。

参会的几位都是底层系统软件领域的老兵——也是开源社区的老朋友。而这场讨论会的主角——泽文，带来了一个让所有人都「挺吃惊」的项目。

**tcg-rs：用 AI Agent 从零重写 QEMU TCG 二进制翻译引擎。5 天，4.5 万行 Rust，性能比原版快 30%。**

「我一行代码没有写。」泽文说。

这句话，成了整场讨论的引子。

> 本文整理自社区闭门技术讨论会录音稿，关于 tcg-rs 的 Agent 完整对话记录已开源到 github 仓库：https://github.com/zevorn/tcg-rs、talk

---

## 01 从 LLVM JIT 到 Houdini：一段前传

故事要从 2024 年说起。

泽文之前在一家做类似 QEMU 的模拟器产品的公司工作，那个模拟器的二进制翻译引擎用的是 LLVM JIT。

「LLVM 翻译出来的代码质量确实高，但速度有点慢。pass 流程太多，太繁琐。」泽文回忆道。

于是团队做了市场调研，发现 Intel 的 Houdini 做得相当不错。Houdini 是 Intel 做的 ARM 到 x86 的二进制翻译器，user mode 下速度能比 QEMU 快 6 倍。

「这个还是挺吓人的。」泽文笑着说，「他们工程化做的很好，很多指令模拟得很细。」

团队花了大量时间研究 Houdini 的设计思路。怎么研究？把 Houdini 跑起来，在进程的内存空间里抓它生成的代码，一条客户机指令翻译成宿主机多少条，代码质量如何，一目了然。

「所以后端指令生成这一块，我是最熟也最感兴趣的。」

当时三个人的团队，花了半年多时间，从零搭起了整个架构框架。那是 ChatGPT 刚出来的年代，AI 还只能对话，没法参与编程。

后来泽文换了工作，去做 AI 芯片的功能模拟器。本职工作对性能没有特别高的要求，但想到后续要跑算子、跑大型应用，性能优化的兴趣又冒了出来。

恰逢 Claude、Codex 这类 Agent 工具兴起。

「我就试着自己也去写一个。」

---

## 02 后端先行：让 AI 从最熟悉的地方开刀

海哥问了一个所有人都好奇的问题：「你是先把整个框架设计好，再让 AI 一个模块一个模块去填吗？」

「没有。」泽文的回答出乎意料，「我没有一上来把所有的顶层设计规划好。」

他的策略是：按照自己之前人工开发的经验路径走。之前团队是先做后端指令生成——因为研究 Houdini 的时候，这块最熟。所以这次也一样，先让 Agent 做 x86 后端。

「我就跟它说，能不能帮我参考 QEMU，给我实现一个 x86 的后端指令生成？」

第一次交付的结果，给了泽文不小的惊喜。虽然没有把所有指令都生成，但已有的部分正确率挺高。而且指令生成这个模块有个天然优势——代码相对「纯粹」。拿到指令的 opcode，按照操作数和目的寄存器编码，发射到内存里。逻辑清晰，又有 QEMU 作为参考。

「它做的还是挺不错的。」

为什么选 Rust？泽文的理由很实在：强类型、编译器提示友好、语法严格。让 AI 去写，编译器可以帮你拦住很多问题。

「基本上能编译过，就七七八八能跑的样子。」

海哥追问：「它中间生成的过程中，有没有自己去迭代？比如编译报错了，会自动返回去修？」

「是的。」泽文解释道，Claude 有一个 Plan Mode——当你给它提出一个比较复杂的需求时，它会先切换到计划模式，把思路和想法告诉你。比如要生成后端指令，它会分析：先实现哪一部分指令，x86 的指令格式是什么样的，分几步走。你可以和它微调优先级，讨论出一个方案，然后它按流程一步步执行。

「这个确实还是挺吃惊的。」海哥感叹，「我原来一直以为你还是给它做好了技术框架，拆分成小单元让它去填。」

---

## 03 设计文档驱动：人只抓两件事

后端做出来以后，泽文发现 Agent 整体上还是按照 QEMU 的风格在做事，但会结合 Rust 的语法做一些优化和调整。他没有一开始给它限定很死，反而觉得这种自由度挺好。

海哥注意到一个细节：「我下午简单看了一下整个代码，无论从模块划分还是命名上，跟 QEMU 非常像。包括里面的 Context、Label、Relocation 这些概念，基本上就是把 C 的逻辑用 Rust 重构了一遍。」

「因为这个项目就叫 Rust for QEMU TCG 嘛。」泽文笑道，「它相当于就是实现了一个 Rust 版本的 TCG。」

那人在这个过程中到底做什么？泽文说得很清楚——只关注两件事：

**第一，设计文档有没有和我的理解对齐。** 每次让 Agent 做完一个功能，他会让它把 Plan Mode 里讨论的结果转化成设计文档，同步迭代进来。包括 Agent 的配置文件，也是一步步同步更新的。

**第二，测试结果有没有和我的预期一致。** 测试的边界和范围，能不能覆盖到我想验证的内容。

「一个是保证 Agent 设计的思路和我一样，第二个就是验证 Agent 给我交付的结果能不能保证功能的正确性和性能。我就把握住这两点就可以了，剩下的就是把活都交给它。」

海哥感叹：「这个理念很超前。」

泽文又补充了一个类比：「其实就和我们芯片设计的思路很像。我们芯片上也有设计文档或者手册，对应有 RTL 版本、功能模拟器版本、性能模拟器版本等等。但大家最终功能的实现，所有的点全部都是对齐同一份文档。」

「我的思路就是这样。」泽文说，「我们要在文档上、在设计上保持一致，然后再通过验证手段去保证 Agent 能够给我全部对齐。」

前期五天时间，生成了几千行、不到一万行代码，泽文还能 cover 住。后面写的改的就太多了，很难一行一行去 review。

「也不要说五天写这么多行代码，五天看这么多行代码我觉得也是很吃力的。」海哥说。

「是的，很头疼。」

所以 4.5 万行代码里，50% 是功能代码，剩下 50% 全部都是测试。这就是泽文的安全网。

---

## 04 测试体系：信心和安全感的来源

除了设计文档，泽文在测试体系上下的功夫最多。

单元测试做得很细，细到 API 级别。这也体现了 Rust 的一个好处——自带的测试框架非常方便，可以直接对 API 写小 case。把 crate include 过来，构造一个很小的输入输出，做一个 check 就可以了。

「当你能够保证 API 粒度的功能正确性的时候，你再往上去搭，给你的信心和安全感就会高很多。」

往上还有模块级别的测试、子系统级别的测试。最关键的一层是差分测试——每一条指令执行的结果，和 QEMU 做对比。

「因为我们参考的就是 QEMU 嘛。如果它执行的行为和结果能和 QEMU 达成一致，那我就可以认为它能够做到和我预期的达成一致了。」

海哥问了一个实际的问题：「有没有出现过指令实现了功能，但测试发现错了的情况？你怎么反馈给它？」

「有的。它自己就会解决。」泽文说，「当它自己写测试的时候，发现功能实现有问题，它自己就会去迭代。实际上大部分功能的错误，都是它自己写测试的时候发现的。而且还很多。」

他笑了笑：「我反而静态 review 代码，没有找出来太多。」

当 Agent 发现编译有错误或者校验没有通过，它会反过来去检查代码。你可以给它一些参考语料——比如在配置里告诉它，你要参考 QEMU 的哪一块源码，要参考哪些手册。模拟 RISC-V 就给它 RISC-V 的指令集手册，后端翻译成 x86 就给它 x86 的手册。它会去理解你原本预期的结果是什么样，然后反推代码逻辑的正确性。

「就这么一轮一轮的去迭代。」

---

## 05 多 Agent 协作：Claude 干活，Codex 审查

泽文在这个项目里不只用了 Claude，还用了 Codex。两个 Agent 怎么协作？

「很简单。在这个终端启动 Claude，在另一个终端启动 Codex。一个活干完了，另一个让它接着去 review 它的代码就行了。」

海哥一开始以为是两个 Agent 在内部做某种协同调度。实际上没那么复杂——就是人在中间做调度，一个写代码，一个审查。

「现在也有一些开源的工具可以做多 Agent 协同，但我觉得那种都太沉重了。」泽文说。Claude 后来也推出了 Team 模式，可以内部生成多个 Agent 同时做任务、做分工。思路是一样的，只是泽文更倾向于用不同厂家的顶级模型一起做。

「感觉效果会好一点。也有可能只是我的想当然。」他笑了笑。

---

## 06 上下文管理：三四轮就换，设计文档是持久记忆

讨论中泽文提到了一个关键细节：**上下文过长时，模型的智力会下降。**

「基本上我可能对话个三四轮，这个上下文我就终止掉了，重新开启一个新的。」

这样做会带来一个问题——Agent 需要重新理解你之前的设计。但因为有设计文档的存在，它不需要把所有代码或资料重新读一遍，只要读设计文档就能快速恢复上下文。

「就能减少一些你的成本，也能提高准确性。」

不过整体而言，随着工程变大，TOKEN 花费还是会激增。前期设计一个问题可能只需要几十 K 的 TOKEN，后期可能需要几百 K。

海哥总结了这个策略：「基本上三四轮完了之后，你就把原来迭代好的、相对完善的设计文档重新给它，再把新的问题和建议给它，让它重新生成。」

「对。或者说我就让它尽可能在一次上下文里，把一个模块或者一个组件给你完成。不要让它跨两个上下文，这样效果会好一点。」

另一个提效手段是 Skill。Claude 有一个能力叫 Skill——把一堆提示词封装在一起，当你触发的任务和这个 Skill 比较相像的时候，它就会用 Skill 里面提供的能力帮你解决问题。

「实际上就是一堆工具和脚本的集合。」泽文解释道，「这样就不用费那么多 TOKEN 了。如果按正常流程，它要理解一遍，自己再写一堆工具，又比较慢。现在调用现成的，帮你实现功能，返回回来就可以了。」

「确定性会更高，效率会提高。」海哥说。

「而且这些 Skill 会不断在你以后的工作中被复用。」

---

## 07 性能优化实战：一个寄存器映射的故事

讨论进入了最硬核的环节。

海哥问了一个关键问题：「你告诉它性能差了，但它有优化方向吗？」

泽文的回答分两层。

第一层是显式指标。比如让它跑 dhrystone，用 time 命令统计运行时间，跑 200 万次、1000 万次，和 QEMU 做对比。时间越少，仿真性能越好。

第二层是代码质量。让 Agent 写一些 trace log，把中间 IR 的翻译情况和代码生成的情况都打印出来，做代码质量检查。也可以让它用 perf 去看热点函数，找到以后去做优化。

「一整套流程，几乎就不用人去参与了。它自己就能给你找出来。这也是让我非常惊喜的一点。」

「这里就体现出来你用的模型是通用智力强不强。」泽文补充道，「它的智力越强，这些手段你是不需要告诉它的。它大量的语料库里面，能够嗅到什么是好代码、什么是坏代码，哪个是好的优化方向、哪个是坏的。」

然后泽文讲了一个具体的优化案例——寄存器固定映射。

思路来自 Houdini：理论上可以把客户机的通用寄存器直接映射到宿主机的寄存器上。这样做指令翻译的时候，就不需要做中间数据的上下文切换，加减乘除可能就 1:1 翻译过去。但问题是宿主机的寄存器数量不够，没办法做一一映射，只能把热点的通用寄存器做固定映射——比如栈指针、ABI 里经常用的传参寄存器。

泽文把这个思路告诉了 Claude，让它去实现。

结果第一轮做完，跑 dhrystone 反而更慢了。

「我就问它，怎么做了固定映射还跑的比原来更慢了呢？」

然后他给了一个新思路：是不是 x86 的通用寄存器不够了？把 SIMD 的寄存器拿过来用一用。因为大部分场景下 SIMD 寄存器不会全部用光，可以拿来做通用寄存器的映射——相当于做了一个 buffer。

「这个功能不太成熟，是一个实验性的优化。给我用一个启动参数去打开它，默认不开启。」

Agent 进入 Plan Mode，做了规划，把所有相关信息了解全以后——跑了 100 多 K 的 TOKEN——给出了一个设计文档，包括用哪些寄存器做映射、关键操作有哪些、会修改哪些文件，标得非常清楚。

伟哥对这个优化很感兴趣：「用 SIMD 寄存器来模拟通用寄存器，这个在外面很少见，包括论文里面可能都很少有。」

「Houdini 里面做了这个优化。」泽文说，「它做 ARM 到 x86 的时候，VPU 的一些模拟是做了 1:1 映射的，有一些通用寄存器是用了 XMM 寄存器做映射的。但它做得很细，应该是做了大量的 profiling 才筛选出来的。」

海哥追问了一个关键问题：「那不怕和向量指令冲突吗？比如它本身也有向量运算，两个之间就会抢。」

「是的，所以要做分析，做大量的实验，选出来那种尽可能用得比较少的去做 buffer。」泽文说，「Houdini 有一部分是固定映射，有一部分是动态分配，交叉着做了一个平衡，没有做得很极端。」

「这种性能收益应该是跟 case 本身很有关联。」伟哥总结道，「高性能计算库你用这种可能就不好了。」

「是的。」

---

## 08 Decode Tree 的 Rust 重生

讨论中还聊到了一个有趣的技术细节。

QEMU 里有一部分代码是靠 Python 脚本（decodetree.py）生成的——解析指令编码的描述文件，生成巨大的 C switch-case 解码器。伟哥问：「这部分你是怎么处理的？直接按照编码来，还是也做了生成器？」

「这个 decode tree 框架，我让 Agent 也用 Rust 给实现出来了。」

Rust 有过程宏（proc macro），相比 Python 生成 C 代码要更优雅。它可以直接在编译期生成 Rust 的解码逻辑，类似于 C 的 switch-case 做指令解码，但不需要外部 Python 依赖。

「而且做这个 decode 的，其实也是一个优化方向。指令编解码的速度也能提高翻译速度。」伟哥补充道。

指令集扩展的管理也做了框架化处理。泽文先让 Agent 按照 RISC-V 的 profile 把指令集扩展的管理框架实现了，参考 QEMU，然后再把基础指令实现完。跑 dhrystone 的时候七七八八都有了，遇到指令错误报错就让 Agent 补全——一方面参考 RISC-V 指令集手册理解指令含义，另一方面参考 QEMU 的现成实现，翻译成 Rust 就行。

「我记得两轮还是三轮就把 dhrystone 跑通了。非常快。」

---

## 09 开源社区与 AI：一场深度对话

讨论会的后半段，话题从技术实现转向了一个更深层的问题。

伟哥提了一个尖锐的问题：「我们从 AI 这边能得到很好的反馈，不一定要把代码发到社区里才能得到好的反馈。而且 AI 的成本更低、收益不比社区小、反馈还很及时。那现在参与开源社区还有没有意义？」

泽文的回答很坦诚。

他是 2024 年底才参与 QEMU 社区的，第一个补丁花了两个月时间，最后是 Peter 帮他 merge 了其中一部分。当时在社区学东西确实学得非常快——从最基础的 code style，到某个模块的认知、演进历史，再到应用场景上的考量，很多细节。

「但到了 2025 年，随着 Agent 工具的发展，它会带来一个有点偏假象的东西。」

什么假象？如果你提前带入了「Agent 做的东西绝大部分都对」的预设，你的信任度就会偏向它。拿过来看代码，风格质量确实很高，看起来就像一个中级工程师、甚至接近高级工程师写的。

「但你会发现，这个初印象就是最好的印象，借用社区成员明阳的话说，叫初见即巅峰。」泽文笑了，「就和相亲一样，我们看到的第一面就是很好的一面。然后你去细究的时候，就会发现内部的一些具体问题了。」

实际上 QEMU 社区对 AI 提交的代码还是相对保守的。完全由 AI 生成的、你没有办法 cover 住的补丁，社区是不会接受的，因为很难评判 AI 生成的代码是否存在侵权问题。但用 AI 做 code review 或者审查测试，没什么问题。

泽文举了一个具体的例子。前几天他参与了 QEMU 社区关于 reset CPU 的讨论。起因是一个比较老的架构，当时实现的时候没有把 CPU 复位功能做上去，导致后面演进时出现了一些小 bug。维护者提交了补丁，但社区的 maintainer 发现——不同架构做 CPU 复位的行为是不一致的。

「这个坏代码的味道，你很难让 Agent 直接去嗅到它。只有很有经验的维护者，像他们都已经做了十年二十年的，对每个架构的演进都了如指掌，才能很敏感地抓到这个点。」

但维护者们讨论的时候也遇到了困境——能感觉到设计上是坏的，但说不清楚哪里坏。泽文就让 Claude 做了一下分析，把所有架构的复位流程逻辑全部分析出来，自己稍微润色了一下，按照社区沟通的形式发了一封邮件去做探讨。

「如果你自己去阅读代码，你很难在短时间内把这些所有的东西给理清楚。但让 Agent 帮我做这个事情，然后我再去验证它，对着它给我的每个环节再去 review 代码，都会比我自己从头做要快很多。」

「但你说我学习到东西了吗？我肯定学习到了。只是说我少了前面那一段可能会走弯路的路程。」

不过社区的讨论也补充了 Agent 分析不出来的信息——比如有人提醒要考虑热插拔的场景。这种上下文，Agent 是没办法直接分析出来的，还是要借助社区的力量。

「所以参与社区的思路是没有问题的。只是说 Agent 可以帮助我们学东西学得更快，解决问题的能力更强。强者恒强。」

---

## 10 AI 的边界：商业级质量仍需人深度参与

讨论会接近尾声，伟哥问了最后一个问题：如果要做商业级、产品级的稳定性和兼容性——比如跑 OpenJDK、跑 GLIBC 这种大型测试集——AI 能不能搞定？

泽文的回答很直接：「本质上还是要靠人去做质量保证。AI 没有办法做到很完备，它只能帮你解决某些局部的问题。全局性的整体保障，还是要人去做深度参与。」

他举了一个例子：Claude CLI 这个工具本身的开发，90% 的代码都是由 AI 来写的。但你说这 90% 都是 AI 写，人就没有参与了吗？不可能的。Claude CLI 的团队还是会带着三五个人，不停地让 AI 去做预期的功能、做预期的验证和测试。人的环节非常关键，没有办法替代。

「它就是一个非常好用的工具，但它代替不了人。」

需求是最先传递到团队、传递到工程师这儿的。你还需要去把它表达给 AI，用 AI 也好，用手动写代码也好，只是形式上的差异。整个流程环节，目前还是没有办法完全让 AI 去做。

伟哥总结道：「或许还是得我们给它提供整体的思路和一些创新的工具，让它再结合代码框架去验证。」

---

## 11 写在最后：新工具催生新机遇

伟哥在讨论中提到了一个趋势性的观察：随着 AI 的发展，人在开源社区里的价值是在上升还是在下降？

泽文的回答很有意思。

「其实这就有点像汽车相对于马，高铁相对于汽车。好用的工具肯定会一定程度上冲击原有的协作框架和体系。但新的东西又会在一定程度上催生出新的领军人物。」

他举了一个例子：Linux 内核的 BPF 模块已经引入了 Agent 去帮他们做补丁的 review。这个动作不一定会减少开发者的存在价值，它更多的正向作用是帮核心 maintainer 降低 review 成本，让他们把更多精力放在更重要的模块或者社区治理上。

对于新手来说，Agent 做 review 反而是降低门槛的方式——你至少有一个渠道能看到补丁从提交到被接受的快速反馈路径。当一个 maintainer 维护的模块比较多的时候，你发一个补丁上去，回复链路可能很长。有了这些工具的协助，可以帮你降低整体门槛。

「可能短时间内会影响大部分人的信心，但长期趋势来说，我觉得可能是提振大家信心的。」

还有一个更深层的问题：开源社区有很多重要的基础设施，维护者可能就一两个人，甚至是无偿维护的。整个互联网的基石，可能就下面一块小石头在支撑。如果有 AI Agent 工具的加持，可以降低维护成本，甚至给它做重写——就像做 tcg-rs 一样——去迸发新的活力。

「这种生产力的进步，是可以去缓解这种问题的。」

「主要还是要调整心态吧。」泽文最后笑着说。

讨论会在轻松的氛围中结束。海哥说：「期待后面泽文你还可以写成一些连载。」

「我觉得这也是个很好的实践。」

---

> 作者：泽文，格维开源社区负责人，QEMU 活跃贡献者。深耕体系结构、AI 芯片模拟器开发与基础软件性能优化。
>
> 项目地址：[tcg-rs](https://github.com/patchfx/tcg-rs)
