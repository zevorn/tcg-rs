# 5 天，4.5 万行 Rust，性能超 QEMU 30%：我用 Claude + Codex 零代码重写了一个二进制翻译引擎

> 当 AI 能独立完成系统级底层软件工程，人类程序员的价值到底在哪里？

---

## 起因：一条让编译器圈集体焦虑的新闻

前段时间，Claude 搞了个大新闻——零代码，完全靠 Agent 写出了一个媲美 GCC 的编译器。虽然编译质量不如 GCC，但它能完整编译 Linux 这样庞大且复杂的开源项目。

消息一出，开源社区里做编译器开发的朋友们集体焦虑了一波。「大家都要失业了」的声音此起彼伏。

我当时就想：与其焦虑，不如亲自试试。

我选了一个比编译器更底层、更硬核的方向——**QEMU 的 TCG（Tiny Code Generator）二进制动态翻译引擎**，用 AI Agent 从零重写它。

结果？**5 天时间，4.5 万行 Rust 代码，816 个测试用例，性能比原版 QEMU TCG 快 30%。**

---

## 先看效果：差点翻车的现场演示

在正式讲技术之前，先看结果。

我们用 dhrystone（一个经典的 CPU 基准测试程序）来对比。先跑 QEMU 原版：

```bash
$ TIMEFORMAT=%R; time qemu-riscv64 dhrystone
# 0.4~0.5 秒
```

再跑我们用 Rust 重写的版本：

```bash
$ TIMEFORMAT=%R; time target/release/tcg-riscv64 dhrystone
# 0.32 秒
```

差点翻车——第一次跑的时候还慢了一丢丢，吓出一身冷汗。多跑几次取平均值，稳定在比原版快 20%~30% 的水平。

当然，这里要诚实地说：我们目前只实现了原版大约 30% 的功能。功能少，代码路径短，快一点是符合预期的。如果实现了全部功能还比原版慢，那才是白重写了。

---

## 什么是二进制动态翻译？用人话讲

用一句话解释：**给你一本日语书，我把你想读的章节实时用中文朗读出来。**

两个关键词：**翻译** + **实时**。

技术定义：在运行时，把客户机架构（比如 RISC-V）的机器码，动态翻译成宿主机架构（比如 x86-64）的机器码，然后直接执行。

翻译流水线长这样：

```
Guest Binary → 前端解码 → TCG IR（中间表示）→ 优化 → 后端代码生成 → Host Binary
                                    ↓
                            TranslationBlock 缓存
```

所谓的中间表示（IR），你可以理解成一种比机器码更抽象、更通用的「指令」。它没有明显的体系结构倾向，有点像编程语言——有局部变量、全局变量、标签跳转、加减乘除。只不过 TCG IR 比 LLVM IR 简单得多，够用就行。

和解释执行的区别在哪？解释执行每次都要解码、查表、调函数，循环往复。而 JIT 编译是一次翻译、缓存结果，下次直接跳到生成的机器码执行。性能差距是数量级的。

**为什么这件事很难？**

- 你得精通客户机和宿主机两种体系结构
- 你得实现寄存器分配器、指令编码器、IR 优化器
- 你得管理 JIT 代码缓冲区（mmap/mprotect）
- 你得保证多线程翻译的安全性
- QEMU TCG 核心源码约 5 万行 C，经过 20 年打磨

我当时学 x86 指令编码格式，花了一两个月才摸清楚。时间久了不用，又忘了。

但 AI 不会忘。

---

## 我的 AI 工具链：一个架构师 + 一个码农

整个项目主要用了两个 AI 工具：

**Claude Code（主力开发者，「Claude 哥」）**

定位：架构师 + 核心逻辑开发。负责架构决策、核心功能实现、设计文档编写。我把 QEMU 的 C 源码和 x86 手册（第二卷、第三卷）全部丢给它，它就能生成质量相当不错的 Rust 实现。

**Codex（协作者，「Codex 哥」）**

定位：批量代码生成 + 代码 Review。主要负责模式化代码的批量生成（比如 184 条 RISC-V 指令的翻译函数），以及单元测试的编写。

**我（产品经理）**

我就是带着两个数字员工把这事干出来的那个人。我来定义做什么、为什么做，AI 负责怎么做。

为什么用两个 Agent？一个原因是让它们交叉验证，互相卷，效果比单个 Agent 好很多。另一个原因嘛——**主要是 Claude 太贵了**，哈哈。

---

## 架构全景：10 个 Crate 的 Rust 工程

我们用 Rust 的 Crate 来划分模块，整体架构如下：

| Crate | 职责 | 对标 QEMU |
|-------|------|----------|
| **tcg-core** | IR 定义：158 个 opcodes、temps、labels、TB 元数据 | `tcg.h` / `tcg-opc.h` |
| **tcg-backend** | 优化器 + 寄存器分配 + x86-64 代码生成 | `tcg.c` / `optimize.c` |
| **tcg-frontend** | RISC-V 解码 + 184 条指令翻译 | `target/riscv/translate.c` |
| **tcg-exec** | MTTCG 执行循环 + TB 缓存管理 | `cpu-exec.c` |
| **tcg-linux-user** | ELF 加载 + Linux syscall 仿真 | `linux-user/` |
| **decode** | .decode 解析器 + Rust 解码器生成 | `decodetree.py` |
| **disas** | RISC-V 反汇编器 | — |
| **tests** | 816 个分层测试 | — |

值得一提的是 `decode` 这个 Crate。QEMU 用 Python 脚本（`decodetree.py`）根据指令编码配置文件生成巨大的 C switch-case 解码器，避免每个架构都手写一遍。但 C 本身不具备这种元编程能力，只能靠外部脚本。

Rust 有过程宏（proc macro），可以在编译期直接生成代码。所以我们用 Rust 原生实现了 decodetree 的功能，不需要外部 Python 依赖，而且调试起来方便得多。

---

## 六大技术亮点：AI 做得漂亮的地方

### 亮点一：统一多态 Opcode——减少 40% 的操作码

QEMU 的 C 实现中，同一个操作（比如加法）会按数据位宽拆成多个 opcode：`add_i32`、`add_i64`。这导致 opcode 数量膨胀到约 250 个。

tcg-rs 的方案是统一起来：

```rust
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
#[repr(u8)]
pub enum Opcode {
    Mov = 0,
    Add,        // 统一的加法，不区分 i32/i64
    Sub,
    Mul,
    // ... 共 158 个
}
```

类型信息不再编码在 opcode 里，而是通过 `Op::op_type` 字段传递。结果：**158 个 opcode vs QEMU 的 ~250 个，减少 40%**。优化器和后端的逻辑也因此统一了，不用再写两套。

### 亮点二：约束驱动的寄存器分配器——零分支的通用设计

寄存器分配是代码生成的核心难题之一。比如 x86 的加法指令，目的寄存器和源寄存器可以是同一个；但有些指令不行。这些规则因体系结构而异。

QEMU 的做法是在分配器里写大量的 per-opcode switch-case。tcg-rs 的做法是声明式约束：

```rust
// 每个 opcode 只需一行约束声明
Opcode::Add => o1_i2_alias(REGS_ALL, REGS_ALL, REGS_ALL),
Opcode::Mul => o1_i2(REGS_ALL, REGS_ALL, REGS_ALL),
Opcode::Div => o1_i2_alias_fixed(RAX, REGS_ALL, REGS_ALL),
```

分配器本身完全泛型，不包含任何 opcode 特定的逻辑。添加新 opcode？在约束表加一行就行，分配器代码零改动。

这个设计是 Claude 自己想出来的，我觉得比 QEMU 的实现要优雅不少。

### 亮点三：x86-64 指令编码器——AI 读手册比人快

x86 的指令编码格式是出了名的复杂。REX 前缀、ModR/M 字节、SIB 字节、各种转义序列……我当年学这个花了一两个月。

我把 QEMU 的 C 实现和 Intel x86 手册（第二卷、第三卷）全部丢给 Claude，它生成了一个 1,232 行的完整 GPR 指令编码器。采用 QEMU 风格的 `u32` 操作码常量，高位编码前缀标志：

```rust
pub const P_EXT: u32   = 0x100;  // 0x0F 前缀
pub const P_REXW: u32  = 0x1000; // REX.W = 1

pub const OPC_ADD_GvEv: u32  = 0x03;
pub const OPC_MOVZBL: u32    = 0xB6 | P_EXT;
pub const OPC_MOVSLQ: u32    = 0x63 | P_REXW;
```

还有一个智能立即数编码的细节——`mov` 指令会自动选择最短编码：

```
val == 0       → xor reg, reg      (2 字节)
val ≤ u32::MAX → mov r32, imm32    (5 字节)
val 可符号扩展  → mov r64, imm32sx  (7 字节)
其他           → movabs r64, imm64  (10 字节)
```

这种细节直接影响生成代码的体积和 I-cache 效率。AI 做这种「照着手册和参考实现生成代码」的事情，确实做得非常好。

### 亮点四：单遍 IR 优化器——经典编译优化的 Rust 实现

优化器在活跃性分析之前运行，635 行代码实现了四种经典优化：

- **常量折叠**：`3 + 5` 直接算出 `8`，不生成加法指令
- **拷贝传播**：追踪 `copy_of` 链，消除冗余拷贝
- **代数简化**：`x + 0 → x`、`x * 1 → x`、`x ^ x → 0`
- **分支常量折叠**：条件恒真/恒假的分支直接消除

这些都是 QEMU `optimize.c` 里已经实现过的，照着 C 版本用 Rust 重写一遍。AI 做这种「有参考实现的等价翻译」效率极高。

### 亮点五：MTTCG 多线程执行引擎

执行引擎是整个翻译器的心脏。核心循环的逻辑是：查找 TB → 执行 → 处理退出 → 链接 → 循环。

tcg-rs 实现了完整的 MTTCG（Multi-Threaded TCG），包括：

- **双层 TB 查找**：每 CPU 的 JumpCache（4096 项直接映射）+ 全局哈希表（32K 桶）
- **`goto_tb` 直接链路 patch**：TB 之间零开销跳转，不回执行循环
- **两个额外的热路径优化**：
  - `next_tb_hint`：链路上复用上次的目标 TB，减少重复查找
  - `exit_target`：原子缓存间接跳转的目标 TB

可以用 `TCG_STATS=1` 环境变量查看实际的命中率数据：

```bash
$ TCG_STATS=1 target/release/tcg-riscv64 dhrystone
```

会输出 TB 命中率、链路 patch 次数、hint 使用率等统计信息。

### 亮点六：184 条 RISC-V 指令翻译

前端实现了 RISC-V 的用户态子集：RV64I（基础整数）、RV64M（乘除）、RV64F/D（单精度/双精度浮点）、RVC（压缩指令）。

2,317 行代码，210 个翻译函数。采用 QEMU 风格的 `BinOp` 函数指针模式来减少重复代码：

```rust
type BinOp = fn(&mut Context, Type, TempIdx, TempIdx, TempIdx)
    -> TempIdx;
```

为什么要实现浮点指令？因为 dhrystone 链接了 libc，`printf` 打印浮点数时会用到浮点指令和原子指令。为了跑通 dhrystone，这些零零散散的指令都得补上。

---

## 质量保障：816 个测试 + 差分测试

我实际上没有特别仔细去看 AI 生成的每一行代码。我只是对代码风格做了一些 review。那怎么确保正确性？靠测试。

### 测试金字塔

```
          ┌────────────┐
          │ Guest 程序  │  18 tests — dhrystone/hello 端到端
          ├────────────┤
          │  Difftest  │  35 tests — 逐指令对比 QEMU
          ├────────────┤
          │ 前端指令测试 │  91 tests — 编解码 + IR 验证
          ├────────────┤
          │  集成测试   │  105 tests — 完整流水线
     ┌────┴────────────┴────┐
     │      单元测试         │  567 tests — API 级别
     └──────────────────────┘
```

### 差分测试：终极安全网

这是我觉得最关键的一层保障。工作流程：

1. tcg-rs 执行一条 RISC-V 指令，记录寄存器结果
2. 生成等价的 RISC-V 汇编，交叉编译成 ELF
3. 用 `qemu-riscv64` 执行，解析寄存器转储
4. 对比两边的结果

**我不信任 AI 的实现，我信任 QEMU 的实现。** 让 Rust 版本的结果去对齐 QEMU，这是我能想到的最好的正确性保障手段。

当然，QEMU 本身也可能有 bug。但作为一个经过 20 年打磨的项目，它的可信度远高于 5 天写出来的代码。

单元测试主要是 Codex 写的。两个 Agent 交叉工作——一个写代码，一个写测试——互相卷，效果比让一个 Agent 干所有事情好很多。

---

## 小工具：给 AI 装上「观测探针」

为了验证 AI 做的事情到底对不对，我参考 LLVM 的 tools 思路，做了几个调试用的小工具。

### tcg-irdump：前端 IR 转储

把客户机 ELF 翻译成 IR 并查看：

```bash
$ tcg-irdump hello --arch riscv64 --count 5
```

输出每条客户机指令对应的 IR。比如一条 RISC-V 的 16 位压缩加法立即数指令 `c.addi s4, 8`，会翻译成：

```
c.addi s4, 8
  add_i64  tmp1, gpr[s4], $0x8
  mov_i64  gpr[s4], tmp1
```

这样你就能直观地看到前端翻译得对不对。

### tcg-irbackend：后端代码生成

从 IR 生成 x86-64 机器码并反汇编：

```bash
$ tcg-irdump hello --emit-bin hello.tcgir
$ tcg-irbackend hello.tcgir --disas
```

可以看到完整的流水线：Guest ELF → IR → x86-64 汇编。

这些工具给了我更多的观测途径，让我能在不逐行审查代码的情况下，验证 AI 的输出是否正确。

---

## 性能分析：为什么快 30%？

先泼一盆冷水：我们目前只实现了原版大约 30% 的功能，功能少、代码路径短，快一点是符合预期的。

但具体的性能优势来源还是值得分析：

- **Rust 零成本抽象**：枚举 match、trait dispatch 在编译期解析，运行时零开销
- **统一 opcode 减少分派开销**：158 个 vs 250 个，switch-case 更短
- **约束驱动分配器减少分支预测失败**：通用路径，无 per-opcode 分支
- **热路径优化**：`next_tb_hint` + `exit_target` 减少哈希查找
- **更紧凑的代码生成**：智能立即数编码减少指令体积
- **专注用户态**：不需要支持全系统模拟的复杂性

### 诚实地说局限性

- 目前只支持 RISC-V → x86-64 单一翻译路径，QEMU 支持 20+ 架构对
- syscall 仿真覆盖有限，只够跑 dhrystone 和简单程序
- 全局 `translate_lock` 在高并发下仍是瓶颈
- 总体功能只有原版的 30%，通用性远不如 QEMU

---

## AI 开发工作流复盘：5 天的时间线

整个开发过程大约 5 天，花费 200 美元。

第一件事就是通过 `claude` 命令进入 CLI，让 Claude 帮我生成整个项目模板，包括 Cargo 工程配置。一开始经验不足，让它实现的功能比较简单，然后一步步迭代，逐渐把工程做到现在的规模。

为了让我和 AI 的理解保持对齐，我让它写了大量的设计文档——98 KB，5 个设计文档。**如果你后面看这个仓库，主要看设计文档就行，源码看不看无所谓。**

### AI 擅长的部分

**1. C → Rust 的对照翻译**

给它 QEMU 的 C 代码，它能生成质量很高的等价 Rust 实现。这一点做得比人类还好——它不会遗漏边界情况，不会因为疲劳而犯低级错误。

**2. 模式化代码的批量生成**

184 条 RISC-V 指令的翻译函数，每条指令的模式都差不多。AI 做这种重复性工作效率极高，而且不会因为枯燥而出错。

**3. 测试编写**

AI 写测试的细致程度超出预期。你只需要不停提需求，它会尽可能满足你，而且能想到比人类更多的边界情况。

**4. 设计文档**

你可以让它用尽可能通俗清晰的描述来告诉你它做了什么。也可以在 Claude 的 Plan Mode 里，把具体要设计的模块、侧重点讨论清楚了再让它干活。

### AI 不擅长的部分

**1. 性能调优的方向判断**

AI 可以帮你做 profiling，但具体往哪个方向优化，这个决策权还是在人手里。如果你不做探索，只让它盲目测试，很容易陷入死循环。

**2. 边界情况和全局最优**

AI 能做到局部最优，但全局最优的经验不如人类。主要问题是随着上下文变长，AI 会变傻，会忘掉重要的细节。人类更擅长提炼关键特征。

**3. 复杂的多线程 bug**

竞态条件这种微妙的 bug，AI 很容易走死胡同。如果你不给提示，它没办法按照预期的方向去解决问题。

**4. 最终的正确性验证**

兜底还是要靠人类。AI 聪明到什么程度？社区有朋友反馈，Claude 可以通过「作弊」的方式给你造成功能已实现的假象——为了达成你的目标，有点不择手段。所以必须通过测试手段来监控 AI 做的事情到底对不对。

---

## 经验总结：CLAUDE.md 驱动的开发模式

针对 AI 不擅长的问题，有一些实践经验：

**1. 维护好你的 CLAUDE.md 和 AGENTS.md**

这两个文件是项目的「大脑」。你在迭代过程中遇到的边边角角的细节，都可以丢进去。甚至不用自己写，告诉 AI 让它写也行。随着工程复杂度变化，这两个文件要不断迭代，保持和当前进展同步。

**2. 差分测试是 AI 生成代码的安全网**

前提是你有一个可信的参考实现。如果没有参照物，这事确实很难做。

**3. 人类的核心价值**

不是 prompt engineering——那只是一个技能。最关键的是两点：

- **定义问题的能力**：怎么提一个好的问题，怎么把需求描述清楚
- **验证结果的能力**：怎么 cover 住 AI 做的事情是不是正确的

你的知识和技术栈要和 AI 在同一个水平，甚至比它高一点，才能把这个事情做好。AI 会倒逼你去学习更多——不然你就会被它欺骗。

---

## 数据总结

| 指标 | 数值 |
|------|------|
| 开发时间 | 5 天 |
| 总代码量 | 45,195 行 Rust |
| 测试数量 | 816 个 |
| Crate 数量 | 10 个 |
| IR Opcodes | 158 个 |
| 指令翻译 | 184 条 RISC-V 指令 |
| 设计文档 | 98 KB（5 篇） |
| 性能对比 | 超 QEMU TCG 20%~30% |
| 花费 | ~200 美元 |

其中核心代码大约 1 万行，占原版 QEMU TCG 5 万行的 20%。其余大部分是前端指令翻译和测试代码（测试占了约 50%）。

---

## 未来方向

说实话，后面怎么做我还没完全想好。但有几个方向比较感兴趣：

- **实现 x86 前端 + RISC-V 后端**：做一个 x86 → RISC-V 的翻译器，有点像 box64 做的事情
- **替换 QEMU 原版的 C 实现**：这个目标比较远，但技术上是可行的
- **更多客户架构支持**：ARM、MIPS 等
- **SIMD/向量指令后端**：目前只有 GPR 指令，SSE/AVX 还没做

---

## 写在最后

4.5 万行代码，一天一万行。你很难想象正常开发能做到这个速度。

但这 4.5 万行里，可能核心的就 1 万行。AI 已经能够独立完成系统级底层软件工程了，但做得好不好、正确性怎么样，还是需要人来参与和把关。

**AI 有能力做这个事情了。关键不在于它写代码的能力，而在于我们定义问题和验证结果的能力。**

整个项目花了 200 美元（中转站 RMB 1:1 充值美刀）。但还是挺贵的~。

---

> 项目地址：[tcg-rs](https://github.com/patchfx/tcg-rs)（欢迎 Star）
>
> 作者：泽文 | B 站分享 QEMU/虚拟化/LLVM/WiFi 引擎相关内容
